{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da7ce875",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "54fd0f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797220cb",
   "metadata": {},
   "source": [
    "# Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dbc1fc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Longitude is represented from 0.0 - 359.0\n",
    "# To convert them:\n",
    "def convert_lon(lon: float) -> float:\n",
    "    \"\"\"\n",
    "    Converts longitudes from -180 to 180 degree range format to 0 to 360 degree \n",
    "    range.\n",
    "\n",
    "    Parameters:\n",
    "        lon (float): The longitude in the -180 to 180 degree format.\n",
    "    \n",
    "    Returns:\n",
    "        converted_longitude (float): The equivalent longitude in the 0 to 360 \n",
    "        degree format.\n",
    "    \"\"\"\n",
    "    return float((lon + 360) % 360)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ea8337b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_grads_ctl(ctl_file_path):\n",
    "    \"\"\"\n",
    "    Parses a GrADS .ctl file to extract metadata for dimensions and variables from \n",
    "    the referenced .dat binary file.\n",
    "\n",
    "    Parameters:\n",
    "        ctl_file_path (str): The path to the GrADS .ctl file.\n",
    "\n",
    "    Returns:\n",
    "        metadata (dict): A dictionary containing parsed metadata like 'dset', 'undef',\n",
    "              'title', 'xdef', 'ydef', 'tdef', 'zdef', 'num_vars', and 'variables'.\n",
    "    \"\"\"\n",
    "    metadata = {}\n",
    "    original_lines = []\n",
    "    with open(ctl_file_path, 'r') as f:\n",
    "        original_lines = f.readlines()\n",
    "\n",
    "    lines_processed = [line.strip().lower() for line in original_lines]\n",
    "    \n",
    "    vars_start_index = -1\n",
    "\n",
    "    for i, line_content in enumerate(lines_processed):\n",
    "        if line_content.startswith(\"dset\"):\n",
    "            metadata['dset'] = original_lines[i].strip().split()[1].strip('^')\n",
    "        elif line_content.startswith(\"undef\"):\n",
    "            metadata['undef'] = float(original_lines[i].strip().split()[1])\n",
    "        elif line_content.startswith(\"title\"):\n",
    "            metadata['title'] = original_lines[i].strip().split(' ', 1)[1]\n",
    "        elif line_content.startswith(\"options\"):\n",
    "            metadata['byte_order'] = line_content.split()[1]\n",
    "        elif line_content.startswith(\"xdef\"):\n",
    "            parts = original_lines[i].strip().split()\n",
    "            metadata['xdef'] = {'count': int(parts[1]), 'type': parts[2], 'start': float(parts[3]), 'increment': float(parts[4])}\n",
    "        elif line_content.startswith(\"ydef\"):\n",
    "            parts = original_lines[i].strip().split()\n",
    "            metadata['ydef'] = {'count': int(parts[1]), 'type': parts[2], 'start': float(parts[3]), 'increment': float(parts[4])}\n",
    "        elif line_content.startswith(\"tdef\"):\n",
    "            parts = original_lines[i].strip().split()\n",
    "            metadata['tdef'] = {'count': int(parts[1]), 'type': parts[2], 'start_str': parts[3], 'increment_str': parts[4]}\n",
    "        elif line_content.startswith(\"zdef\"):\n",
    "            parts = original_lines[i].strip().split()\n",
    "            metadata['zdef'] = {'count': int(parts[1]), 'type': parts[2], 'start': float(parts[3]), 'increment': float(parts[4])}\n",
    "        elif line_content.startswith(\"vars\"):\n",
    "            metadata['num_vars'] = int(line_content.split()[1])\n",
    "            metadata['variables'] = []\n",
    "            vars_start_index = i\n",
    "\n",
    "    if vars_start_index != -1:\n",
    "        for j in range(vars_start_index + 1, len(original_lines)):\n",
    "            sub_line = original_lines[j].strip()\n",
    "            if sub_line.lower() == \"endvars\":\n",
    "                break\n",
    "            \n",
    "            parts = sub_line.split()\n",
    "            if parts:\n",
    "                var_name = parts[0]\n",
    "                description = ''\n",
    "                if '**' in parts:\n",
    "                    desc_start_idx = parts.index('**') + 1\n",
    "                    description = \" \".join(parts[desc_start_idx:])\n",
    "                else:\n",
    "                    if len(parts) > 2:\n",
    "                        description = \" \".join(parts[2:]) \n",
    "                \n",
    "                metadata['variables'].append({'name': var_name, 'description': description.strip()})\n",
    "    \n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "246463ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset(ds: xr.Dataset, left_lon: float, right_lon: float, up_lat: float, \n",
    "           down_lat: float) -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Subsets an xarray Dataset containing georeferenced data to a given bound.\n",
    "\n",
    "    Parameters:\n",
    "        ds (xarray.Dataset): An xarray Dataset to be subset.\n",
    "        left_lon (float): The western longitude bound.\n",
    "        right_lon (float): The eastern longitude bound.\n",
    "        up_lat (float): The northern latitude bound.\n",
    "        down_lat (float): The southern latitude bound.\n",
    "    \n",
    "    Returns:\n",
    "        subset_dataset (xarray.Dataset): A new xarray Dataset containing the subset \n",
    "        data within the latitude and longitude bounding box.\n",
    "    \"\"\"\n",
    "    subset_ds = ds.sel(lat= slice(down_lat, up_lat), lon= slice(left_lon, right_lon))\n",
    "    return subset_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d439261b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For masking negative values to 0\n",
    "def mask(ds: xr.Dataset) -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Masks negative values in the xarray Dataset's skill values (sk1 to sk8) to zero.\n",
    "\n",
    "    Parameters:\n",
    "        ds (xarray.Dataset): An xarray Dataset to be masked.\n",
    "\n",
    "    Returns:\n",
    "        masked_dataset (xarray.Dataset): A new xarray Dataset where negative skill \n",
    "        values have been masked to zero.\n",
    "    \"\"\"\n",
    "    masked_ds = ds.copy(deep=True)\n",
    "\n",
    "    for skill in [\"sk1\", \"sk2\", \"sk3\", \"sk4\", \"sk5\", \"sk6\", \"sk7\", \"sk8\"]:\n",
    "        masked_ds[skill] = masked_ds[skill].where(ds[skill] >= 0.0, 0.0)\n",
    "\n",
    "    return masked_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db07784",
   "metadata": {},
   "source": [
    "If not interested in masking and subsetting the data to a given region, comment out those lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b92c91a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_grads_to_netcdf(ctl_file_path: str, output_nc_file: str, \n",
    "                            left_lon: float = None, right_lon: float = None, \n",
    "                            up_lat: float = None, down_lat: float = None):\n",
    "    \"\"\"\n",
    "    Converts a GrADS binary data file (.dat) associated with a .ctl file into a \n",
    "    NetCDF (.nc) file. Optional Parameters are for subsetting and masking the \n",
    "    xarray Dataset.\n",
    "\n",
    "    Parameters:\n",
    "        ctl_file_path (str): The path to the GrADS .ctl control file.\n",
    "        output_nc_file (str): The desired path for the output NetCDF file.\n",
    "        left_lon (float): Optional. The western longitude bound.\n",
    "        right_lon (float): Optional. The eastern longitude bound.\n",
    "        up_lat (float): Optional. The northern latitude bound.\n",
    "        down_lat (float): Optional. The southern latitude bound.\n",
    "    \"\"\"\n",
    "    metadata = parse_grads_ctl(ctl_file_path)\n",
    "\n",
    "    data_filename_from_ctl = metadata['dset']\n",
    "    data_file_dir = os.path.dirname(ctl_file_path)\n",
    "    data_file_path = os.path.join(data_file_dir, data_filename_from_ctl)\n",
    "\n",
    "    # 1. Infer spatial dimensions and coordinates\n",
    "    x_coords = np.arange(metadata['xdef']['start'],\n",
    "                         metadata['xdef']['start'] + metadata['xdef']['count'] * metadata['xdef']['increment'],\n",
    "                         metadata['xdef']['increment'])\n",
    "    y_coords = np.arange(metadata['ydef']['start'],\n",
    "                         metadata['ydef']['start'] + metadata['ydef']['count'] * metadata['ydef']['increment'],\n",
    "                         metadata['ydef']['increment'])\n",
    "    z_coords = np.arange(metadata['zdef']['start'],\n",
    "                         metadata['zdef']['start'] + metadata['zdef']['count'] * metadata['zdef']['increment'],\n",
    "                         metadata['zdef']['increment'])\n",
    "\n",
    "    # 2. Time parsing: Convert GrADS time format to pandas DatetimeIndex\n",
    "    try:\n",
    "        start_date = pd.to_datetime(metadata['tdef']['start_str'], format='%HZ%d%b%Y')\n",
    "    except ValueError:\n",
    "        print(f\"Warning: Could not parse start date '{metadata['tdef']['start_str']}' with '%HZ%d%b%Y'. Attempting generic parse.\")\n",
    "        start_date = pd.to_datetime(metadata['tdef']['start_str']) \n",
    "\n",
    "    freq_map = {'1yr': 'YS', '1mo': 'MS', '1dy': 'D', '1hr': 'H', \n",
    "                '1mn': 'min', '1sc': 'S'}\n",
    "    time_freq = metadata['tdef']['increment_str'].lower()\n",
    "    if time_freq in freq_map:\n",
    "        time_freq = freq_map[time_freq]\n",
    "    else:\n",
    "        print(f\"Warning: Could not directly map GrADS time increment '{metadata['tdef']['increment_str']}' to pandas frequency. Trying simple replacement.\")\n",
    "        if 'yr' in time_freq: time_freq = time_freq.replace('yr', 'Y')\n",
    "        elif 'mo' in time_freq: time_freq = time_freq.replace('mo', 'M')\n",
    "        elif 'dy' in time_freq: time_freq = time_freq.replace('dy', 'D')\n",
    "        elif 'hr' in time_freq: time_freq = time_freq.replace('hr', 'H')\n",
    "        elif 'mn' in time_freq: time_freq = time_freq.replace('mn', 'min')\n",
    "        elif 'sc' in time_freq: time_freq = time_freq.replace('sc', 'S')\n",
    "        else:\n",
    "            print(f\"Error: Unable to determine pandas frequency from '{metadata['tdef']['increment_str']}'. Defaulting to 'D'. This may cause incorrect time coordinates.\")\n",
    "            time_freq = 'D'\n",
    "\n",
    "    time_coords = pd.date_range(start=start_date, periods=metadata['tdef']['count'], freq=time_freq)\n",
    "\n",
    "    # 3. Determine data type and byte order\n",
    "    base_dtype = np.float32 \n",
    "    endian = '<' if metadata.get('byte_order', 'little_endian').lower() == 'little_endian' else '>'\n",
    "    data_dtype = np.dtype(f\"{endian}f{base_dtype().itemsize}\")\n",
    "\n",
    "    # 4. Calculate total expected number of values (elements) from CTL\n",
    "    total_expected_elements = (metadata['tdef']['count'] *\n",
    "                               metadata['zdef']['count'] *\n",
    "                               metadata['num_vars'] *\n",
    "                               metadata['ydef']['count'] *\n",
    "                               metadata['xdef']['count'])\n",
    "    \n",
    "    expected_data_bytes = total_expected_elements * base_dtype().itemsize\n",
    "\n",
    "    try:\n",
    "        actual_file_size_bytes = os.path.getsize(data_file_path)\n",
    "        \n",
    "        # Calculate potential header/footer bytes\n",
    "        excess_bytes = actual_file_size_bytes - expected_data_bytes\n",
    "\n",
    "        if excess_bytes < 0:\n",
    "            raise ValueError(f\"Error: Data file '{data_file_path}' is smaller than expected. \"\n",
    "                             f\"Expected {expected_data_bytes} bytes but found {actual_file_size_bytes} bytes. Conversion aborted.\")\n",
    "        elif excess_bytes > 0:\n",
    "            print(f\"Warning: Data file '{data_file_path}' contains {excess_bytes} excess bytes.\")\n",
    "            print(\"Assuming these excess bytes are at the *beginning* of the file (a header) and will be skipped.\")\n",
    "            bytes_to_skip_at_start = excess_bytes\n",
    "        else:\n",
    "            bytes_to_skip_at_start = 0\n",
    "            print(f\"File size matches expected data size: {actual_file_size_bytes} bytes.\")\n",
    "\n",
    "        # Read the binary data: Open the file and skip potential header, then read data\n",
    "        with open(data_file_path, 'rb') as f:\n",
    "            if bytes_to_skip_at_start > 0:\n",
    "                f.seek(bytes_to_skip_at_start)\n",
    "                print(f\"Skipped {bytes_to_skip_at_start} bytes at the beginning of the file.\")\n",
    "            \n",
    "            raw_data = np.fromfile(f, dtype=data_dtype, count=total_expected_elements)\n",
    "\n",
    "            # Verify that we actually read the expected number of elements\n",
    "            if raw_data.size != total_expected_elements:\n",
    "                 raise ValueError(f\"Failed to read expected number of elements after skipping header. \"\n",
    "                                  f\"Expected {total_expected_elements}, but read {raw_data.size}.\")\n",
    "\n",
    "        # Reshape the raw data.\n",
    "        data_shape = (\n",
    "            metadata['tdef']['count'],\n",
    "            metadata['zdef']['count'],\n",
    "            metadata['num_vars'], \n",
    "            metadata['ydef']['count'],\n",
    "            metadata['xdef']['count']\n",
    "        )\n",
    "        # By trial and error, GrADS data is seen C-ordered (row-major)\n",
    "        reshaped_data = raw_data.reshape(data_shape, order='C') \n",
    "\n",
    "        # 5. Create DataArrays for each variable and populate the Dataset\n",
    "        data_vars = {}\n",
    "        for i, var_info in enumerate(metadata['variables']):\n",
    "            var_data_array = reshaped_data[:, :, i, :, :]\n",
    "            # Missing data is changed to NaN for compatibility with Pandas and xarray\n",
    "            var_data_array[var_data_array == metadata['undef']] = np.nan\n",
    "            squeezed_data = var_data_array.squeeze()\n",
    "            \n",
    "            dims = []\n",
    "            if metadata['tdef']['count'] > 1: dims.append('time')\n",
    "            if metadata['zdef']['count'] > 1: dims.append('z')\n",
    "            dims.append('lat')\n",
    "            dims.append('lon')\n",
    "\n",
    "            current_data_array_coords = {}\n",
    "            if 'time' in dims:\n",
    "                current_data_array_coords['time'] = time_coords\n",
    "            if 'z' in dims:\n",
    "                current_data_array_coords['z'] = z_coords\n",
    "            current_data_array_coords['lat'] = y_coords\n",
    "            current_data_array_coords['lon'] = x_coords\n",
    "\n",
    "            data_vars[var_info['name']] = xr.DataArray(\n",
    "                squeezed_data,\n",
    "                coords=current_data_array_coords,\n",
    "                dims=dims,\n",
    "                name=var_info['name'],\n",
    "                attrs={'long_name': var_info['description'], 'units': 'kg/m^2/s'}\n",
    "            )\n",
    "\n",
    "        # 6. Create the xarray Dataset\n",
    "        ds = xr.Dataset(\n",
    "            data_vars=data_vars,\n",
    "            coords={\n",
    "                'time': time_coords,\n",
    "                'z': z_coords,\n",
    "                'lat': y_coords,\n",
    "                'lon': x_coords\n",
    "            },\n",
    "            attrs={'title': metadata.get('title', 'Converted from GrADS binary')}\n",
    "        )\n",
    "\n",
    "        ####### OPTIONAL HELPERS FOR SUBSETING AND MASKING #######\n",
    "        ds = subset(ds, left_lon, right_lon, up_lat, down_lat)\n",
    "        ds = mask(ds)\n",
    "        ##########################################################\n",
    "\n",
    "        # Ensure the output directory exists\n",
    "        output_dir = os.path.dirname(output_nc_file)\n",
    "        if output_dir and not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "\n",
    "        # 7. Save to NetCDF\n",
    "        ds.to_netcdf(output_nc_file)\n",
    "        print(f\"Successfully converted '{ctl_file_path}' to '{output_nc_file}'\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Data file '{data_file_path}' not found. Please ensure the .dat file exists and the `dset` path in your .ctl is correct relative to the .ctl file.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during conversion: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add682b1",
   "metadata": {},
   "source": [
    "# Constants and Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9fbded5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "292.0\n",
      "296.0\n",
      "19.0\n",
      "17.0\n",
      "prod/to_convert/\n",
      "prod/converted/\n"
     ]
    }
   ],
   "source": [
    "# The box is lon: -68 - -64, \n",
    "#            lat: 17.5 N - 19 N\n",
    "\n",
    "# lat and lon values are float32 but the step is 1.0\n",
    "\n",
    "left_lon = -68.0\n",
    "right_lon = -64.0\n",
    "up_lat = 19.0\n",
    "down_lat = 17.0\n",
    "input_dir = \"prod/to_convert/\"\n",
    "output_dir = \"prod/converted/\"\n",
    "\n",
    "# Longitude is represented from 0.0 - 359.0\n",
    "left_lon = convert_lon(left_lon)\n",
    "right_lon = convert_lon(right_lon)\n",
    "\n",
    "print(left_lon, right_lon, up_lat, down_lat, input_dir, output_dir, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcbd951",
   "metadata": {},
   "source": [
    "# Main Call to Convert and Transform Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9768c33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    try:\n",
    "        for filename in os.listdir(input_dir):\n",
    "            if filename.endswith(\".ctl\"):\n",
    "                # Input file path\n",
    "                ctl_file_path = os.path.join(input_dir, filename)\n",
    "\n",
    "                # Output file path\n",
    "                output_nc_filename = filename.replace(\".ctl\", \".nc\")\n",
    "                output_nc_file = os.path.join(output_dir, output_nc_filename)\n",
    "\n",
    "                # Call\n",
    "                convert_grads_to_netcdf(ctl_file_path, output_nc_file, left_lon, \n",
    "                                        right_lon, up_lat, down_lat)\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Directory: '{input_dir}' not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Expection ocurred: {e}.\")\n",
    "\n",
    "    print(f\"Iterated over '{input_dir}' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8ff5844c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File size matches expected data size: 2085120 bytes.\n",
      "Successfully converted 'prod/to_convert/AprIC_nmme_precip_skill.ctl' to 'prod/converted/AprIC_nmme_precip_skill.nc'\n",
      "File size matches expected data size: 2085120 bytes.\n",
      "Successfully converted 'prod/to_convert/AugIC_nmme_precip_skill.ctl' to 'prod/converted/AugIC_nmme_precip_skill.nc'\n",
      "File size matches expected data size: 2085120 bytes.\n",
      "Successfully converted 'prod/to_convert/DecIC_nmme_precip_skill.ctl' to 'prod/converted/DecIC_nmme_precip_skill.nc'\n",
      "File size matches expected data size: 2085120 bytes.\n",
      "Successfully converted 'prod/to_convert/FebIC_nmme_precip_skill.ctl' to 'prod/converted/FebIC_nmme_precip_skill.nc'\n",
      "File size matches expected data size: 2085120 bytes.\n",
      "Successfully converted 'prod/to_convert/JanIC_nmme_precip_skill.ctl' to 'prod/converted/JanIC_nmme_precip_skill.nc'\n",
      "File size matches expected data size: 2085120 bytes.\n",
      "Successfully converted 'prod/to_convert/JulIC_nmme_precip_skill.ctl' to 'prod/converted/JulIC_nmme_precip_skill.nc'\n",
      "File size matches expected data size: 2085120 bytes.\n",
      "Successfully converted 'prod/to_convert/JunIC_nmme_precip_skill.ctl' to 'prod/converted/JunIC_nmme_precip_skill.nc'\n",
      "File size matches expected data size: 2085120 bytes.\n",
      "Successfully converted 'prod/to_convert/MarIC_nmme_precip_skill.ctl' to 'prod/converted/MarIC_nmme_precip_skill.nc'\n",
      "File size matches expected data size: 2085120 bytes.\n",
      "Successfully converted 'prod/to_convert/MayIC_nmme_precip_skill.ctl' to 'prod/converted/MayIC_nmme_precip_skill.nc'\n",
      "File size matches expected data size: 2085120 bytes.\n",
      "Successfully converted 'prod/to_convert/NovIC_nmme_precip_skill.ctl' to 'prod/converted/NovIC_nmme_precip_skill.nc'\n",
      "File size matches expected data size: 2085120 bytes.\n",
      "Successfully converted 'prod/to_convert/OctIC_nmme_precip_skill.ctl' to 'prod/converted/OctIC_nmme_precip_skill.nc'\n",
      "File size matches expected data size: 2085120 bytes.\n",
      "Successfully converted 'prod/to_convert/SepIC_nmme_precip_skill.ctl' to 'prod/converted/SepIC_nmme_precip_skill.nc'\n",
      "Iterated over 'prod/to_convert/' directory.\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
