{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88e3d1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import os\n",
    "import struct # For reading binary record markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "941decd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_grads_ctl(ctl_file_path):\n",
    "    \"\"\"\n",
    "    Parses a GrADS .ctl file to extract metadata for dimensions and variables.\n",
    "\n",
    "    Args:\n",
    "        ctl_file_path (str): The path to the GrADS .ctl file.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing parsed metadata like 'dset', 'undef',\n",
    "              'xdef', 'ydef', 'tdef', 'zdef', 'num_vars', and 'variables'.\n",
    "    \"\"\"\n",
    "    metadata = {}\n",
    "    original_lines = []\n",
    "    with open(ctl_file_path, 'r') as f:\n",
    "        original_lines = f.readlines()\n",
    "\n",
    "    lines_processed = [line.strip().lower() for line in original_lines]\n",
    "    \n",
    "    vars_start_index = -1\n",
    "\n",
    "    for i, line_content in enumerate(lines_processed):\n",
    "        if line_content.startswith(\"dset\"):\n",
    "            metadata['dset'] = original_lines[i].strip().split()[1].strip('^')\n",
    "        elif line_content.startswith(\"undef\"):\n",
    "            metadata['undef'] = float(original_lines[i].strip().split()[1])\n",
    "        elif line_content.startswith(\"title\"):\n",
    "            metadata['title'] = original_lines[i].strip().split(' ', 1)[1]\n",
    "        elif line_content.startswith(\"options\"):\n",
    "            metadata['byte_order'] = line_content.split()[1]\n",
    "        elif line_content.startswith(\"xdef\"):\n",
    "            parts = original_lines[i].strip().split()\n",
    "            metadata['xdef'] = {'count': int(parts[1]), 'type': parts[2], 'start': float(parts[3]), 'increment': float(parts[4])}\n",
    "        elif line_content.startswith(\"ydef\"):\n",
    "            parts = original_lines[i].strip().split()\n",
    "            metadata['ydef'] = {'count': int(parts[1]), 'type': parts[2], 'start': float(parts[3]), 'increment': float(parts[4])}\n",
    "        elif line_content.startswith(\"tdef\"):\n",
    "            parts = original_lines[i].strip().split()\n",
    "            metadata['tdef'] = {'count': int(parts[1]), 'type': parts[2], 'start_str': parts[3], 'increment_str': parts[4]}\n",
    "        elif line_content.startswith(\"zdef\"):\n",
    "            parts = original_lines[i].strip().split()\n",
    "            metadata['zdef'] = {'count': int(parts[1]), 'type': parts[2], 'start': float(parts[3]), 'increment': float(parts[4])}\n",
    "        elif line_content.startswith(\"vars\"):\n",
    "            metadata['num_vars'] = int(line_content.split()[1])\n",
    "            metadata['variables'] = []\n",
    "            vars_start_index = i\n",
    "\n",
    "    if vars_start_index != -1:\n",
    "        for j in range(vars_start_index + 1, len(original_lines)):\n",
    "            sub_line = original_lines[j].strip()\n",
    "            if sub_line.lower() == \"endvars\":\n",
    "                break\n",
    "            \n",
    "            parts = sub_line.split()\n",
    "            if parts:\n",
    "                var_name = parts[0]\n",
    "                description = ''\n",
    "                if '**' in parts:\n",
    "                    desc_start_idx = parts.index('**') + 1\n",
    "                    description = \" \".join(parts[desc_start_idx:])\n",
    "                else:\n",
    "                    if len(parts) > 2:\n",
    "                        description = \" \".join(parts[2:]) \n",
    "                \n",
    "                metadata['variables'].append({'name': var_name, 'description': description.strip()})\n",
    "    \n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88be9f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_grads_to_netcdf(ctl_file_path, output_nc_file):\n",
    "    \"\"\"\n",
    "    Converts a GrADS binary data file (.dat) associated with a .ctl file\n",
    "    into a NetCDF (.nc) file using xarray. This version assumes the data\n",
    "    is a raw binary stream of floats, potentially with a leading header.\n",
    "\n",
    "    Args:\n",
    "        ctl_file_path (str): The path to the GrADS .ctl control file.\n",
    "        output_nc_file (str): The desired path for the output NetCDF file.\n",
    "    \"\"\"\n",
    "    metadata = parse_grads_ctl(ctl_file_path)\n",
    "\n",
    "    data_filename_from_ctl = metadata['dset']\n",
    "    data_file_dir = os.path.dirname(ctl_file_path)\n",
    "    data_file_path = os.path.join(data_file_dir, data_filename_from_ctl)\n",
    "\n",
    "    # 1. Infer spatial dimensions and coordinates\n",
    "    x_coords = np.arange(metadata['xdef']['start'],\n",
    "                         metadata['xdef']['start'] + metadata['xdef']['count'] * metadata['xdef']['increment'],\n",
    "                         metadata['xdef']['increment'])\n",
    "    y_coords = np.arange(metadata['ydef']['start'],\n",
    "                         metadata['ydef']['start'] + metadata['ydef']['count'] * metadata['ydef']['increment'],\n",
    "                         metadata['ydef']['increment'])\n",
    "    z_coords = np.arange(metadata['zdef']['start'],\n",
    "                         metadata['zdef']['start'] + metadata['zdef']['count'] * metadata['zdef']['increment'],\n",
    "                         metadata['zdef']['increment'])\n",
    "\n",
    "    # 2. Time parsing: Convert GrADS time format to pandas DatetimeIndex\n",
    "    try:\n",
    "        start_date = pd.to_datetime(metadata['tdef']['start_str'], format='%HZ%d%b%Y')\n",
    "    except ValueError:\n",
    "        print(f\"Warning: Could not parse start date '{metadata['tdef']['start_str']}' with '%HZ%d%b%Y'. Attempting generic parse.\")\n",
    "        start_date = pd.to_datetime(metadata['tdef']['start_str']) \n",
    "\n",
    "    freq_map = {'1yr': 'YS', '1mo': 'MS', '1dy': 'D', '1hr': 'H', \n",
    "                '1mn': 'min', '1sc': 'S'}\n",
    "    time_freq = metadata['tdef']['increment_str'].lower()\n",
    "    if time_freq in freq_map:\n",
    "        time_freq = freq_map[time_freq]\n",
    "    else:\n",
    "        print(f\"Warning: Could not directly map GrADS time increment '{metadata['tdef']['increment_str']}' to pandas frequency. Trying simple replacement.\")\n",
    "        if 'yr' in time_freq: time_freq = time_freq.replace('yr', 'Y')\n",
    "        elif 'mo' in time_freq: time_freq = time_freq.replace('mo', 'M')\n",
    "        elif 'dy' in time_freq: time_freq = time_freq.replace('dy', 'D')\n",
    "        elif 'hr' in time_freq: time_freq = time_freq.replace('hr', 'H')\n",
    "        elif 'mn' in time_freq: time_freq = time_freq.replace('mn', 'min')\n",
    "        elif 'sc' in time_freq: time_freq = time_freq.replace('sc', 'S')\n",
    "        else:\n",
    "            print(f\"Error: Unable to determine pandas frequency from '{metadata['tdef']['increment_str']}'. Defaulting to 'D'. This may cause incorrect time coordinates.\")\n",
    "            time_freq = 'D'\n",
    "\n",
    "    time_coords = pd.date_range(start=start_date, periods=metadata['tdef']['count'], freq=time_freq)\n",
    "\n",
    "    # 3. Determine data type and byte order\n",
    "    base_dtype = np.float32 \n",
    "    endian = '<' if metadata.get('byte_order', 'little_endian').lower() == 'little_endian' else '>'\n",
    "    data_dtype = np.dtype(f\"{endian}f{base_dtype().itemsize}\")\n",
    "\n",
    "    # 4. Calculate total expected number of values (elements) from CTL\n",
    "    total_expected_elements = (metadata['tdef']['count'] *\n",
    "                               metadata['zdef']['count'] *\n",
    "                               metadata['num_vars'] *\n",
    "                               metadata['ydef']['count'] *\n",
    "                               metadata['xdef']['count'])\n",
    "    \n",
    "    expected_data_bytes = total_expected_elements * base_dtype().itemsize\n",
    "\n",
    "    try:\n",
    "        actual_file_size_bytes = os.path.getsize(data_file_path)\n",
    "        \n",
    "        # Calculate potential header/footer bytes\n",
    "        excess_bytes = actual_file_size_bytes - expected_data_bytes\n",
    "\n",
    "        if excess_bytes < 0:\n",
    "            raise ValueError(f\"Error: Data file '{data_file_path}' is smaller than expected. \"\n",
    "                             f\"Expected {expected_data_bytes} bytes but found {actual_file_size_bytes} bytes. Conversion aborted.\")\n",
    "        elif excess_bytes > 0:\n",
    "            print(f\"Warning: Data file '{data_file_path}' contains {excess_bytes} excess bytes.\")\n",
    "            print(\"Assuming these excess bytes are at the *beginning* of the file (a header) and will be skipped.\")\n",
    "            bytes_to_skip_at_start = excess_bytes\n",
    "        else:\n",
    "            bytes_to_skip_at_start = 0\n",
    "            print(f\"File size matches expected data size: {actual_file_size_bytes} bytes.\")\n",
    "\n",
    "\n",
    "        # Read the binary data: Open the file and skip potential header, then read data\n",
    "        with open(data_file_path, 'rb') as f:\n",
    "            if bytes_to_skip_at_start > 0:\n",
    "                f.seek(bytes_to_skip_at_start)\n",
    "                print(f\"Skipped {bytes_to_skip_at_start} bytes at the beginning of the file.\")\n",
    "            \n",
    "            raw_data = np.fromfile(f, dtype=data_dtype, count=total_expected_elements)\n",
    "            \n",
    "            # Print a sample of raw data for diagnostic purposes\n",
    "            print(f\"Sample of raw data (first 20 elements): {raw_data[:20]}\")\n",
    "\n",
    "            # Verify that we actually read the expected number of elements\n",
    "            if raw_data.size != total_expected_elements:\n",
    "                 raise ValueError(f\"Failed to read expected number of elements after skipping header. \"\n",
    "                                  f\"Expected {total_expected_elements}, but read {raw_data.size}.\")\n",
    "\n",
    "\n",
    "        # Reshape the raw data.\n",
    "        # GrADS binary is commonly X fastest, then Y, then Z, then Variable, then Time.\n",
    "        # This translates to Fortran 'column-major' order for (Time, Z, Variables, Lat, Lon)\n",
    "        # HOWEVER, the 'repeated grid' symptom often means the data is actually C-order (row-major).\n",
    "        # We are now trying 'C' order.\n",
    "        data_shape = (\n",
    "            metadata['tdef']['count'],\n",
    "            metadata['zdef']['count'],\n",
    "            metadata['num_vars'], \n",
    "            metadata['ydef']['count'],\n",
    "            metadata['xdef']['count']\n",
    "        )\n",
    "        reshaped_data = raw_data.reshape(data_shape, order='C') # Changed from 'F' to 'C'\n",
    "\n",
    "        # 5. Create DataArrays for each variable and populate the Dataset\n",
    "        data_vars = {}\n",
    "        for i, var_info in enumerate(metadata['variables']):\n",
    "            var_data_array = reshaped_data[:, :, i, :, :]\n",
    "            var_data_array[var_data_array == metadata['undef']] = np.nan\n",
    "            squeezed_data = var_data_array.squeeze()\n",
    "            \n",
    "            dims = []\n",
    "            if metadata['tdef']['count'] > 1: dims.append('time')\n",
    "            if metadata['zdef']['count'] > 1: dims.append('z')\n",
    "            dims.append('lat')\n",
    "            dims.append('lon')\n",
    "\n",
    "            current_data_array_coords = {}\n",
    "            if 'time' in dims:\n",
    "                current_data_array_coords['time'] = time_coords\n",
    "            if 'z' in dims:\n",
    "                current_data_array_coords['z'] = z_coords\n",
    "            current_data_array_coords['lat'] = y_coords\n",
    "            current_data_array_coords['lon'] = x_coords\n",
    "\n",
    "            data_vars[var_info['name']] = xr.DataArray(\n",
    "                squeezed_data,\n",
    "                coords=current_data_array_coords,\n",
    "                dims=dims,\n",
    "                name=var_info['name'],\n",
    "                attrs={'long_name': var_info['description'], 'units': 'kg/m^2/s'}\n",
    "            )\n",
    "\n",
    "        # 6. Create the xarray Dataset\n",
    "        ds = xr.Dataset(\n",
    "            data_vars=data_vars,\n",
    "            coords={\n",
    "                'time': time_coords,\n",
    "                'z': z_coords,\n",
    "                'lat': y_coords,\n",
    "                'lon': x_coords\n",
    "            },\n",
    "            attrs={'title': metadata.get('title', 'Converted from GrADS binary')}\n",
    "        )\n",
    "\n",
    "        # Ensure the output directory exists\n",
    "        output_dir = os.path.dirname(output_nc_file)\n",
    "        if output_dir and not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "\n",
    "        # 7. Save to NetCDF\n",
    "        ds.to_netcdf(output_nc_file)\n",
    "        print(f\"Successfully converted '{ctl_file_path}' to '{output_nc_file}'\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Data file '{data_file_path}' not found. Please ensure the .dat file exists and the `dset` path in your .ctl is correct relative to the .ctl file.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during conversion: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "630534b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- How to use ---\n",
    "# IMPORTANT: If you encounter Permission Denied errors, try changing the output directory\n",
    "# to a simple, non-cloud-synced location, e.g., C:/temp/converted_data/\n",
    "\n",
    "# PAY ATTENTION TO HAVE THE OUTPUT WITH .NC \n",
    "\n",
    "ctl_file = \"masking_test/to_convert/JunIC_nmme_precip_skill.ctl\"\n",
    "output_netcdf_file = \"masking_test/converted/JunIC_nmme_precip_skill.nc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f80afcfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File size matches expected data size: 2085120 bytes.\n",
      "Sample of raw data (first 20 elements): [-9.99e+08 -9.99e+08 -9.99e+08 -9.99e+08 -9.99e+08 -9.99e+08 -9.99e+08\n",
      " -9.99e+08 -9.99e+08 -9.99e+08 -9.99e+08 -9.99e+08 -9.99e+08 -9.99e+08\n",
      " -9.99e+08 -9.99e+08 -9.99e+08 -9.99e+08 -9.99e+08 -9.99e+08]\n",
      "Successfully converted 'masking_test/to_convert/JunIC_nmme_precip_skill.ctl' to 'masking_test/converted/JunIC_nmme_precip_skill.nc'\n"
     ]
    }
   ],
   "source": [
    "# Call the conversion function\n",
    "convert_grads_to_netcdf(ctl_file, output_netcdf_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
