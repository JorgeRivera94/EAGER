{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "88e3d1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd # Required for time series handling\n",
    "import os # For path manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "941decd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_grads_ctl(ctl_file_path):\n",
    "    \"\"\"\n",
    "    Parses a GrADS .ctl file to extract metadata for dimensions and variables.\n",
    "\n",
    "    Args:\n",
    "        ctl_file_path (str): The path to the GrADS .ctl file.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing parsed metadata like 'dset', 'undef',\n",
    "              'xdef', 'ydef', 'tdef', 'zdef', 'num_vars', and 'variables'.\n",
    "    \"\"\"\n",
    "    metadata = {}\n",
    "    original_lines = [] # To keep original line content for accurate splitting/parsing\n",
    "    with open(ctl_file_path, 'r') as f:\n",
    "        original_lines = f.readlines()\n",
    "\n",
    "    # Create a lowercased, stripped version of lines for keyword matching\n",
    "    lines_processed = [line.strip().lower() for line in original_lines]\n",
    "    \n",
    "    vars_start_index = -1 # To mark the beginning of the VARS section\n",
    "\n",
    "    for i, line_content in enumerate(lines_processed):\n",
    "        if line_content.startswith(\"dset\"):\n",
    "            # Use original_lines[i] to get the exact string for splitting\n",
    "            metadata['dset'] = original_lines[i].strip().split()[1].strip('^')\n",
    "        elif line_content.startswith(\"undef\"):\n",
    "            metadata['undef'] = float(original_lines[i].strip().split()[1])\n",
    "        elif line_content.startswith(\"title\"):\n",
    "            # Capture the full title string after 'title' keyword\n",
    "            metadata['title'] = original_lines[i].strip().split(' ', 1)[1]\n",
    "        elif line_content.startswith(\"options\"):\n",
    "            metadata['byte_order'] = original_lines[i].strip().split()[1]\n",
    "        elif line_content.startswith(\"xdef\"):\n",
    "            parts = original_lines[i].strip().split()\n",
    "            metadata['xdef'] = {'count': int(parts[1]), 'type': parts[2], 'start': float(parts[3]), 'increment': float(parts[4])}\n",
    "        elif line_content.startswith(\"ydef\"):\n",
    "            parts = original_lines[i].strip().split()\n",
    "            metadata['ydef'] = {'count': int(parts[1]), 'type': parts[2], 'start': float(parts[3]), 'increment': float(parts[4])}\n",
    "        elif line_content.startswith(\"tdef\"):\n",
    "            parts = original_lines[i].strip().split()\n",
    "            metadata['tdef'] = {'count': int(parts[1]), 'type': parts[2], 'start_str': parts[3], 'increment_str': parts[4]}\n",
    "        elif line_content.startswith(\"zdef\"):\n",
    "            parts = original_lines[i].strip().split()\n",
    "            metadata['zdef'] = {'count': int(parts[1]), 'type': parts[2], 'start': float(parts[3]), 'increment': float(parts[4])}\n",
    "        elif line_content.startswith(\"vars\"):\n",
    "            metadata['num_vars'] = int(line_content.split()[1])\n",
    "            metadata['variables'] = []\n",
    "            vars_start_index = i # Store the index of the 'vars' line\n",
    "\n",
    "    # After iterating through all lines, parse variables using the stored index\n",
    "    if vars_start_index != -1:\n",
    "        for j in range(vars_start_index + 1, len(original_lines)):\n",
    "            sub_line = original_lines[j].strip()\n",
    "            if sub_line.lower() == \"endvars\":\n",
    "                break # Stop when 'endvars' is encountered\n",
    "            \n",
    "            parts = sub_line.split()\n",
    "            if parts: # Ensure the line is not empty\n",
    "                var_name = parts[0]\n",
    "                description = ''\n",
    "                # Attempt to find description after '**'\n",
    "                if '**' in parts:\n",
    "                    desc_start_idx = parts.index('**') + 1\n",
    "                    description = \" \".join(parts[desc_start_idx:])\n",
    "                else:\n",
    "                    # Heuristic for cases without '**': take remaining parts as description\n",
    "                    # This might need adjustment based on specific CTL file formats\n",
    "                    if len(parts) > 2: # e.g., 'sk1 0 59,1,0'\n",
    "                        description = \" \".join(parts[2:]) \n",
    "                \n",
    "                metadata['variables'].append({'name': var_name, 'description': description.strip()})\n",
    "    \n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "88be9f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_grads_to_netcdf(ctl_file_path, output_nc_file):\n",
    "    \"\"\"\n",
    "    Converts a GrADS binary data file (.dat) associated with a .ctl file\n",
    "    into a NetCDF (.nc) file using xarray.\n",
    "\n",
    "    Args:\n",
    "        ctl_file_path (str): The path to the GrADS .ctl control file.\n",
    "        output_nc_file (str): The desired path for the output NetCDF file.\n",
    "    \"\"\"\n",
    "    metadata = parse_grads_ctl(ctl_file_path)\n",
    "\n",
    "    # Construct the data file path (assumes .dat is in the same directory)\n",
    "    data_file_path = os.path.join(os.path.dirname(ctl_file_path), metadata['dset'])\n",
    "    \n",
    "    # Check if the data file specified in dset uses '^' relative path.\n",
    "    # If dset starts with '^', it means it's relative to the ctl file location.\n",
    "    # Otherwise, it could be an absolute path or relative to the CWD.\n",
    "    # We already handled '^' by stripping it during metadata parsing.\n",
    "    # Ensure data_file_path is correct if 'dset' had a full path.\n",
    "    if not os.path.isabs(metadata['dset']) and not metadata['dset'].startswith('^'):\n",
    "        # If it's a simple filename without '^' or absolute path, assume it's in the same dir\n",
    "        data_file_path = os.path.join(os.path.dirname(ctl_file_path), metadata['dset'])\n",
    "    else:\n",
    "        # If dset was an absolute path or already handled for relative, use it as is\n",
    "        data_file_path = metadata['dset'].strip('^')\n",
    "\n",
    "\n",
    "    # 1. Infer spatial dimensions and coordinates\n",
    "    x_coords = np.arange(metadata['xdef']['start'],\n",
    "                         metadata['xdef']['start'] + metadata['xdef']['count'] * metadata['xdef']['increment'],\n",
    "                         metadata['xdef']['increment'])\n",
    "    y_coords = np.arange(metadata['ydef']['start'],\n",
    "                         metadata['ydef']['start'] + metadata['ydef']['count'] * metadata['ydef']['increment'],\n",
    "                         metadata['ydef']['increment'])\n",
    "    z_coords = np.arange(metadata['zdef']['start'],\n",
    "                         metadata['zdef']['start'] + metadata['zdef']['count'] * metadata['zdef']['increment'],\n",
    "                         metadata['zdef']['increment'])\n",
    "\n",
    "    # 2. Time parsing: Convert GrADS time format to pandas DatetimeIndex\n",
    "    # Handle GrADS specific time formats, e.g., '00Z01jan1991 1yr'\n",
    "    try:\n",
    "        # GrADS date format like '00Z01jan1991' -> '%HZ%d%b%Y'\n",
    "        start_date = pd.to_datetime(metadata['tdef']['start_str'], format='%HZ%d%b%Y')\n",
    "    except ValueError:\n",
    "        # Fallback for other date formats if needed\n",
    "        print(f\"Warning: Could not parse start date '{metadata['tdef']['start_str']}' with '%HZ%d%b%Y'. Attempting generic parse.\")\n",
    "        start_date = pd.to_datetime(metadata['tdef']['start_str']) \n",
    "\n",
    "    # Map GrADS frequency strings to pandas frequency strings\n",
    "    # Add more mappings as necessary based on your .ctl files\n",
    "    freq_map = {'1yr': 'YS', '1mo': 'MS', '1dy': 'D', '1hr': 'H', \n",
    "                '1mn': 'min', '1sc': 'S'} # YS for year start, MS for month start\n",
    "    time_freq = metadata['tdef']['increment_str'].lower()\n",
    "    if time_freq in freq_map:\n",
    "        time_freq = freq_map[time_freq]\n",
    "    else:\n",
    "        print(f\"Warning: Could not directly map GrADS time increment '{metadata['tdef']['increment_str']}' to pandas frequency. Trying simple replacement.\")\n",
    "        # Attempt simple replacements for common patterns\n",
    "        if 'yr' in time_freq: time_freq = time_freq.replace('yr', 'Y')\n",
    "        elif 'mo' in time_freq: time_freq = time_freq.replace('mo', 'M')\n",
    "        elif 'dy' in time_freq: time_freq = time_freq.replace('dy', 'D')\n",
    "        elif 'hr' in time_freq: time_freq = time_freq.replace('hr', 'H')\n",
    "        elif 'mn' in time_freq: time_freq = time_freq.replace('mn', 'min')\n",
    "        elif 'sc' in time_freq: time_freq = time_freq.replace('sc', 'S')\n",
    "        else:\n",
    "            print(f\"Error: Unable to determine pandas frequency from '{metadata['tdef']['increment_str']}'. Defaulting to 'D'. This may cause incorrect time coordinates.\")\n",
    "            time_freq = 'D' # Last resort fallback\n",
    "\n",
    "    time_coords = pd.date_range(start=start_date, periods=metadata['tdef']['count'], freq=time_freq)\n",
    "\n",
    "    # 3. Determine data type and byte order\n",
    "    # Use 'float32' for the base dtype, as GrADS binary data is typically 4-byte floats\n",
    "    base_dtype = np.float32 \n",
    "    \n",
    "    # Determine endianness: '<' for little-endian, '>' for big-endian\n",
    "    endian = '<' if metadata.get('byte_order', 'little_endian').lower() == 'little_endian' else '>'\n",
    "    \n",
    "    # Construct the full dtype string with explicit byte order for robustness\n",
    "    # 'f' for float, then itemsize (e.g., 'f4' for float32)\n",
    "    data_dtype = np.dtype(f\"{endian}f{base_dtype().itemsize}\")\n",
    "\n",
    "    # 4. Calculate total expected number of values and reshape\n",
    "    # GrADS binary data is commonly organized as X (longitude) fastest, then Y (latitude),\n",
    "    # then Z (vertical level), then Variable, then Time (slowest).\n",
    "    # So the flat file content conceptually follows this order:\n",
    "    # for t in times:\n",
    "    #   for z in z_levels:\n",
    "    #     for var in variables:\n",
    "    #       for y in latitudes:\n",
    "    #         for x in longitudes:\n",
    "    #           read_value()\n",
    "\n",
    "    # The total number of elements in the binary file expected by the CTL\n",
    "    total_expected_elements = (metadata['tdef']['count'] *\n",
    "                               metadata['zdef']['count'] *\n",
    "                               metadata['num_vars'] * # Each variable block\n",
    "                               metadata['ydef']['count'] *\n",
    "                               metadata['xdef']['count'])\n",
    "\n",
    "    try:\n",
    "        # Get the actual file size in bytes\n",
    "        actual_file_size_bytes = os.path.getsize(data_file_path)\n",
    "        expected_file_size_bytes = total_expected_elements * base_dtype().itemsize\n",
    "\n",
    "        if actual_file_size_bytes > expected_file_size_bytes:\n",
    "            print(f\"Warning: Data file '{data_file_path}' is larger than expected.\")\n",
    "            print(f\"Expected data size: {expected_file_size_bytes} bytes ({total_expected_elements} elements)\")\n",
    "            print(f\"Actual file size: {actual_file_size_bytes} bytes ({actual_file_size_bytes / base_dtype().itemsize} elements)\")\n",
    "            print(\"Proceeding by reading only the expected number of data elements.\")\n",
    "        elif actual_file_size_bytes < expected_file_size_bytes:\n",
    "            raise ValueError(f\"Error: Data file '{data_file_path}' is smaller than expected. \"\n",
    "                             f\"Expected {expected_file_size_bytes} bytes but found {actual_file_size_bytes} bytes. Conversion aborted.\")\n",
    "\n",
    "        # Read the binary data, reading only the expected number of elements\n",
    "        # This will ignore any trailing bytes if the file is larger than expected\n",
    "        raw_data = np.fromfile(data_file_path, dtype=data_dtype, count=total_expected_elements)\n",
    "        \n",
    "        # This check should now always pass if count is used, but keep for robustness if count changes\n",
    "        if raw_data.size != total_expected_elements:\n",
    "             # This should ideally not happen if count is specified correctly.\n",
    "             # It might indicate an underlying issue with np.fromfile or a very strange file.\n",
    "             raise ValueError(f\"Internal error: Read {raw_data.size} elements, but expected {total_expected_elements} after specifying count.\")\n",
    "\n",
    "\n",
    "        # Reshape the raw data.\n",
    "        # The reshape order is (Time, Z, Variables, Lat, Lon)\n",
    "        # Using 'F' (Fortran-like) order because X is the fastest varying dimension in GrADS binary.\n",
    "        data_shape = (\n",
    "            metadata['tdef']['count'],\n",
    "            metadata['zdef']['count'],\n",
    "            metadata['num_vars'], \n",
    "            metadata['ydef']['count'],\n",
    "            metadata['xdef']['count']\n",
    "        )\n",
    "        reshaped_data = raw_data.reshape(data_shape, order='F') \n",
    "\n",
    "        # 5. Create DataArrays for each variable and populate the Dataset\n",
    "        data_vars = {}\n",
    "        for i, var_info in enumerate(metadata['variables']):\n",
    "            # Slice out the data for each variable from the reshaped array\n",
    "            # The structure is (time, z, variable_index, lat, lon)\n",
    "            var_data_array = reshaped_data[:, :, i, :, :]\n",
    "\n",
    "            # Apply undef value mask\n",
    "            var_data_array[var_data_array == metadata['undef']] = np.nan\n",
    "\n",
    "            # Use .squeeze() to remove any singleton dimensions (e.g., Z if zdef 1)\n",
    "            # This makes the DataArray more natural for 2D or 3D data\n",
    "            squeezed_data = var_data_array.squeeze()\n",
    "            \n",
    "            # Dynamically determine dimensions for the squeezed data\n",
    "            # This logic is already correct\n",
    "            dims = []\n",
    "            if metadata['tdef']['count'] > 1: dims.append('time')\n",
    "            if metadata['zdef']['count'] > 1: dims.append('z')\n",
    "            dims.append('lat')\n",
    "            dims.append('lon')\n",
    "\n",
    "            # Dynamically create the coords dictionary for each DataArray\n",
    "            # Only include coordinates for dimensions that are actually present\n",
    "            current_data_array_coords = {}\n",
    "            if 'time' in dims:\n",
    "                current_data_array_coords['time'] = time_coords\n",
    "            if 'z' in dims:\n",
    "                current_data_array_coords['z'] = z_coords\n",
    "            current_data_array_coords['lat'] = y_coords\n",
    "            current_data_array_coords['lon'] = x_coords\n",
    "\n",
    "            data_vars[var_info['name']] = xr.DataArray(\n",
    "                squeezed_data,\n",
    "                coords=current_data_array_coords, # Use the dynamically created coords\n",
    "                dims=dims, # Use dynamically determined dimensions\n",
    "                name=var_info['name'],\n",
    "                attrs={'long_name': var_info['description'], 'units': 'kg/m^2/s'} # Units hardcoded from your CTL\n",
    "            )\n",
    "\n",
    "        # 6. Create the xarray Dataset\n",
    "        # The main Dataset should still contain all original coordinates as full dimensions,\n",
    "        # even if individual DataArrays within it have squeezed dimensions.\n",
    "        ds = xr.Dataset(\n",
    "            data_vars=data_vars,\n",
    "            coords={\n",
    "                'time': time_coords,\n",
    "                'z': z_coords,\n",
    "                'lat': y_coords,\n",
    "                'lon': x_coords\n",
    "            },\n",
    "            attrs={'title': metadata.get('title', 'Converted from GrADS binary')}\n",
    "        )\n",
    "\n",
    "        # Ensure the output directory exists\n",
    "        output_dir = os.path.dirname(output_nc_file)\n",
    "        if output_dir and not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "\n",
    "        # 7. Save to NetCDF\n",
    "        ds.to_netcdf(output_nc_file)\n",
    "        print(f\"Successfully converted '{ctl_file_path}' to '{output_nc_file}'\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Data file '{data_file_path}' not found. Please ensure the .dat file exists and the `dset` path in your .ctl is correct relative to the .ctl file.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during conversion: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc() # Print full traceback for detailed debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "630534b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- How to use ---\n",
    "# Define the paths to your .ctl and desired output .nc files\n",
    "# Ensure 'example_GrADS' directory exists and contains the .ctl and .dat files\n",
    "# Ensure 'converted_test' directory exists or will be created by the script\n",
    "ctl_file = \"example_GrADS/DecIC_nmme_precip_skill.ctl\"\n",
    "output_netcdf_file = \"converted_test/DecIC_nmme_precip_skill.nc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f80afcfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully converted 'example_GrADS/DecIC_nmme_precip_skill.ctl' to 'converted_test/DecIC_nmme_precip_skill.nc'\n"
     ]
    }
   ],
   "source": [
    "# Call the conversion function\n",
    "convert_grads_to_netcdf(ctl_file, output_netcdf_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
