{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88e3d1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import os\n",
    "import struct # For reading binary record markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "941decd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_grads_ctl(ctl_file_path):\n",
    "    \"\"\"\n",
    "    Parses a GrADS .ctl file to extract metadata for dimensions and variables.\n",
    "\n",
    "    Args:\n",
    "        ctl_file_path (str): The path to the GrADS .ctl file.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing parsed metadata like 'dset', 'undef',\n",
    "              'xdef', 'ydef', 'tdef', 'zdef', 'num_vars', and 'variables'.\n",
    "    \"\"\"\n",
    "    metadata = {}\n",
    "    original_lines = []\n",
    "    with open(ctl_file_path, 'r') as f:\n",
    "        original_lines = f.readlines()\n",
    "\n",
    "    lines_processed = [line.strip().lower() for line in original_lines]\n",
    "    \n",
    "    vars_start_index = -1\n",
    "\n",
    "    for i, line_content in enumerate(lines_processed):\n",
    "        if line_content.startswith(\"dset\"):\n",
    "            metadata['dset'] = original_lines[i].strip().split()[1].strip('^')\n",
    "        elif line_content.startswith(\"undef\"):\n",
    "            metadata['undef'] = float(original_lines[i].strip().split()[1])\n",
    "        elif line_content.startswith(\"title\"):\n",
    "            metadata['title'] = original_lines[i].strip().split(' ', 1)[1]\n",
    "        elif line_content.startswith(\"options\"):\n",
    "            metadata['byte_order'] = line_content.split()[1]\n",
    "        elif line_content.startswith(\"xdef\"):\n",
    "            parts = original_lines[i].strip().split()\n",
    "            metadata['xdef'] = {'count': int(parts[1]), 'type': parts[2], 'start': float(parts[3]), 'increment': float(parts[4])}\n",
    "        elif line_content.startswith(\"ydef\"):\n",
    "            parts = original_lines[i].strip().split()\n",
    "            metadata['ydef'] = {'count': int(parts[1]), 'type': parts[2], 'start': float(parts[3]), 'increment': float(parts[4])}\n",
    "        elif line_content.startswith(\"tdef\"):\n",
    "            parts = original_lines[i].strip().split()\n",
    "            metadata['tdef'] = {'count': int(parts[1]), 'type': parts[2], 'start_str': parts[3], 'increment_str': parts[4]}\n",
    "        elif line_content.startswith(\"zdef\"):\n",
    "            parts = original_lines[i].strip().split()\n",
    "            metadata['zdef'] = {'count': int(parts[1]), 'type': parts[2], 'start': float(parts[3]), 'increment': float(parts[4])}\n",
    "        elif line_content.startswith(\"vars\"):\n",
    "            metadata['num_vars'] = int(line_content.split()[1])\n",
    "            metadata['variables'] = []\n",
    "            vars_start_index = i\n",
    "\n",
    "    if vars_start_index != -1:\n",
    "        for j in range(vars_start_index + 1, len(original_lines)):\n",
    "            sub_line = original_lines[j].strip()\n",
    "            if sub_line.lower() == \"endvars\":\n",
    "                break\n",
    "            \n",
    "            parts = sub_line.split()\n",
    "            if parts:\n",
    "                var_name = parts[0]\n",
    "                description = ''\n",
    "                if '**' in parts:\n",
    "                    desc_start_idx = parts.index('**') + 1\n",
    "                    description = \" \".join(parts[desc_start_idx:])\n",
    "                else:\n",
    "                    if len(parts) > 2:\n",
    "                        description = \" \".join(parts[2:]) \n",
    "                \n",
    "                metadata['variables'].append({'name': var_name, 'description': description.strip()})\n",
    "    \n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bbdc0f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_fortran_unformatted_binary(file_path, dtype, initial_byte_order):\n",
    "    \"\"\"\n",
    "    Reads data from a Fortran unformatted binary file, skipping record markers.\n",
    "    It attempts to auto-detect endianness and size (4-byte or 8-byte) of record markers.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the binary file.\n",
    "        dtype (numpy.dtype): The data type of the values (e.g., np.float32).\n",
    "        initial_byte_order (str): The initial guess for data endianness: '<' for little-endian, '>' for big-endian.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: A 1D array of the data values.\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If record markers are consistently invalid or file ends unexpectedly.\n",
    "    \"\"\"\n",
    "    data_elements = []\n",
    "    \n",
    "    # Possible byte orders to try for markers\n",
    "    possible_byte_orders = [initial_byte_order, '<' if initial_byte_order == '>' else '>']\n",
    "    # Possible marker sizes (in bytes)\n",
    "    possible_marker_sizes = [4, 8] # Try int32 then int64\n",
    "    \n",
    "    marker_byte_order = None\n",
    "    marker_size = None\n",
    "\n",
    "    with open(file_path, 'rb') as f:\n",
    "        # First, try to determine the correct marker format by reading the first marker\n",
    "        initial_pos = f.tell()\n",
    "        \n",
    "        for bo in possible_byte_orders:\n",
    "            for ms in possible_marker_sizes:\n",
    "                f.seek(initial_pos) # Reset file pointer for each attempt\n",
    "                marker_bytes_peek = f.read(ms)\n",
    "                if len(marker_bytes_peek) < ms:\n",
    "                    # Not enough bytes for this marker size, try next\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    marker_format_str = bo + ('i' if ms == 4 else 'q') # 'i' for int32, 'q' for int64\n",
    "                    marker_value = struct.unpack(marker_format_str, marker_bytes_peek)[0]\n",
    "                    \n",
    "                    # A plausible marker is positive and not excessively large (e.g., max 1GB for a data block)\n",
    "                    if marker_value > 0 and marker_value < (1 * 1024 * 1024 * 1024):\n",
    "                        marker_byte_order = bo\n",
    "                        marker_size = ms\n",
    "                        print(f\"Detected record marker format: {marker_size}-byte integer, {bo}-endian.\")\n",
    "                        break # Found a plausible format, exit inner loop\n",
    "                except struct.error:\n",
    "                    # Unpacking failed, continue to next format\n",
    "                    pass\n",
    "            if marker_byte_order is not None:\n",
    "                break # Found a plausible format, exit outer loop\n",
    "\n",
    "        if marker_byte_order is None:\n",
    "            raise ValueError(f\"Could not determine plausible Fortran record marker format at byte {initial_pos}. \"\n",
    "                             f\"File might be corrupted or in an unsupported Fortran format.\")\n",
    "        \n",
    "        f.seek(initial_pos) # Reset file pointer to beginning for actual reading loop\n",
    "\n",
    "        while True:\n",
    "            file_pos_before_read = f.tell() # Remember position for potential re-read\n",
    "            \n",
    "            # Read opening record marker\n",
    "            marker1_bytes = f.read(marker_size)\n",
    "            if not marker1_bytes: # End of file\n",
    "                break\n",
    "            if len(marker1_bytes) < marker_size:\n",
    "                raise ValueError(f\"Unexpected end of file while reading record marker 1 at byte {file_pos_before_read}.\")\n",
    "\n",
    "            marker_format_str = marker_byte_order + ('i' if marker_size == 4 else 'q')\n",
    "            marker1 = struct.unpack(marker_format_str, marker1_bytes)[0]\n",
    "            \n",
    "            # Additional check for negative marker, though auto-detection should mostly prevent this\n",
    "            if marker1 < 0 or marker1 > (1 * 1024 * 1024 * 1024): # Large value check, e.g. >1GB is suspect\n",
    "                 raise ValueError(f\"Invalid record marker {marker1} detected at byte {file_pos_before_read} after format detection. \"\n",
    "                                  f\"This indicates file corruption or highly unusual data structure.\")\n",
    "\n",
    "            # Read data block\n",
    "            data_bytes = f.read(marker1)\n",
    "            if len(data_bytes) < marker1:\n",
    "                raise ValueError(f\"Unexpected end of file while reading data block of size {marker1} at byte {f.tell() - marker1}.\")\n",
    "            \n",
    "            # Read closing record marker\n",
    "            marker2_bytes = f.read(marker_size)\n",
    "            if len(marker2_bytes) < marker_size:\n",
    "                raise ValueError(f\"Unexpected end of file while reading record marker 2 at byte {f.tell() - marker_size}.\")\n",
    "            marker2 = struct.unpack(marker_format_str, marker2_bytes)[0]\n",
    "            \n",
    "            if marker1 != marker2:\n",
    "                raise ValueError(f\"Fortran record markers mismatch: {marker1} (start) != {marker2} (end) at record starting byte {file_pos_before_read}. \"\n",
    "                                 f\"Possible file corruption or non-standard Fortran format.\")\n",
    "            \n",
    "            # Convert binary data to numpy array\n",
    "            current_block_data = np.frombuffer(data_bytes, dtype=dtype)\n",
    "            data_elements.append(current_block_data)\n",
    "            \n",
    "    if not data_elements:\n",
    "        print(\"Warning: No data elements found in the binary file. It might be empty or severely malformed.\")\n",
    "        return np.array([], dtype=dtype)\n",
    "\n",
    "    return np.concatenate(data_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88be9f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_grads_to_netcdf(ctl_file_path, output_nc_file):\n",
    "    \"\"\"\n",
    "    Converts a GrADS binary data file (.dat) associated with a .ctl file\n",
    "    into a NetCDF (.nc) file using xarray. This version assumes the data\n",
    "    is a raw binary stream of floats, potentially with a leading header.\n",
    "\n",
    "    Args:\n",
    "        ctl_file_path (str): The path to the GrADS .ctl control file.\n",
    "        output_nc_file (str): The desired path for the output NetCDF file.\n",
    "    \"\"\"\n",
    "    metadata = parse_grads_ctl(ctl_file_path)\n",
    "\n",
    "    data_filename_from_ctl = metadata['dset']\n",
    "    data_file_dir = os.path.dirname(ctl_file_path)\n",
    "    data_file_path = os.path.join(data_file_dir, data_filename_from_ctl)\n",
    "\n",
    "    # 1. Infer spatial dimensions and coordinates\n",
    "    x_coords = np.arange(metadata['xdef']['start'],\n",
    "                         metadata['xdef']['start'] + metadata['xdef']['count'] * metadata['xdef']['increment'],\n",
    "                         metadata['xdef']['increment'])\n",
    "    y_coords = np.arange(metadata['ydef']['start'],\n",
    "                         metadata['ydef']['start'] + metadata['ydef']['count'] * metadata['ydef']['increment'],\n",
    "                         metadata['ydef']['increment'])\n",
    "    z_coords = np.arange(metadata['zdef']['start'],\n",
    "                         metadata['zdef']['start'] + metadata['zdef']['count'] * metadata['zdef']['increment'],\n",
    "                         metadata['zdef']['increment'])\n",
    "\n",
    "    # 2. Time parsing: Convert GrADS time format to pandas DatetimeIndex\n",
    "    try:\n",
    "        start_date = pd.to_datetime(metadata['tdef']['start_str'], format='%HZ%d%b%Y')\n",
    "    except ValueError:\n",
    "        print(f\"Warning: Could not parse start date '{metadata['tdef']['start_str']}' with '%HZ%d%b%Y'. Attempting generic parse.\")\n",
    "        start_date = pd.to_datetime(metadata['tdef']['start_str']) \n",
    "\n",
    "    freq_map = {'1yr': 'YS', '1mo': 'MS', '1dy': 'D', '1hr': 'H', \n",
    "                '1mn': 'min', '1sc': 'S'}\n",
    "    time_freq = metadata['tdef']['increment_str'].lower()\n",
    "    if time_freq in freq_map:\n",
    "        time_freq = freq_map[time_freq]\n",
    "    else:\n",
    "        print(f\"Warning: Could not directly map GrADS time increment '{metadata['tdef']['increment_str']}' to pandas frequency. Trying simple replacement.\")\n",
    "        if 'yr' in time_freq: time_freq = time_freq.replace('yr', 'Y')\n",
    "        elif 'mo' in time_freq: time_freq = time_freq.replace('mo', 'M')\n",
    "        elif 'dy' in time_freq: time_freq = time_freq.replace('dy', 'D')\n",
    "        elif 'hr' in time_freq: time_freq = time_freq.replace('hr', 'H')\n",
    "        elif 'mn' in time_freq: time_freq = time_freq.replace('mn', 'min')\n",
    "        elif 'sc' in time_freq: time_freq = time_freq.replace('sc', 'S')\n",
    "        else:\n",
    "            print(f\"Error: Unable to determine pandas frequency from '{metadata['tdef']['increment_str']}'. Defaulting to 'D'. This may cause incorrect time coordinates.\")\n",
    "            time_freq = 'D'\n",
    "\n",
    "    time_coords = pd.date_range(start=start_date, periods=metadata['tdef']['count'], freq=time_freq)\n",
    "\n",
    "    # 3. Determine data type and byte order\n",
    "    base_dtype = np.float32 \n",
    "    endian = '<' if metadata.get('byte_order', 'little_endian').lower() == 'little_endian' else '>'\n",
    "    data_dtype = np.dtype(f\"{endian}f{base_dtype().itemsize}\")\n",
    "\n",
    "    # 4. Calculate total expected number of values (elements) from CTL\n",
    "    total_expected_elements = (metadata['tdef']['count'] *\n",
    "                               metadata['zdef']['count'] *\n",
    "                               metadata['num_vars'] *\n",
    "                               metadata['ydef']['count'] *\n",
    "                               metadata['xdef']['count'])\n",
    "    \n",
    "    expected_data_bytes = total_expected_elements * base_dtype().itemsize\n",
    "\n",
    "    try:\n",
    "        actual_file_size_bytes = os.path.getsize(data_file_path)\n",
    "        \n",
    "        # Calculate potential header/footer bytes\n",
    "        excess_bytes = actual_file_size_bytes - expected_data_bytes\n",
    "\n",
    "        if excess_bytes < 0:\n",
    "            raise ValueError(f\"Error: Data file '{data_file_path}' is smaller than expected. \"\n",
    "                             f\"Expected {expected_data_bytes} bytes but found {actual_file_size_bytes} bytes. Conversion aborted.\")\n",
    "        elif excess_bytes > 0:\n",
    "            print(f\"Warning: Data file '{data_file_path}' contains {excess_bytes} excess bytes.\")\n",
    "            print(\"Assuming these excess bytes are at the *beginning* of the file (a header) and will be skipped.\")\n",
    "            bytes_to_skip_at_start = excess_bytes\n",
    "        else:\n",
    "            bytes_to_skip_at_start = 0\n",
    "            print(f\"File size matches expected data size: {actual_file_size_bytes} bytes.\")\n",
    "\n",
    "\n",
    "        # Read the binary data: Open the file and skip potential header, then read data\n",
    "        with open(data_file_path, 'rb') as f:\n",
    "            if bytes_to_skip_at_start > 0:\n",
    "                f.seek(bytes_to_skip_at_start)\n",
    "                print(f\"Skipped {bytes_to_skip_at_start} bytes at the beginning of the file.\")\n",
    "            \n",
    "            raw_data = np.fromfile(f, dtype=data_dtype, count=total_expected_elements)\n",
    "            \n",
    "            # Print a sample of raw data for diagnostic purposes\n",
    "            print(f\"Sample of raw data (first 20 elements): {raw_data[:20]}\")\n",
    "\n",
    "            # Verify that we actually read the expected number of elements\n",
    "            if raw_data.size != total_expected_elements:\n",
    "                 raise ValueError(f\"Failed to read expected number of elements after skipping header. \"\n",
    "                                  f\"Expected {total_expected_elements}, but read {raw_data.size}.\")\n",
    "\n",
    "\n",
    "        # Reshape the raw data.\n",
    "        # GrADS binary is commonly X fastest, then Y, then Z, then Variable, then Time.\n",
    "        # This translates to Fortran 'column-major' order for (Time, Z, Variables, Lat, Lon)\n",
    "        # HOWEVER, the 'repeated grid' symptom often means the data is actually C-order (row-major).\n",
    "        # We are now trying 'C' order.\n",
    "        data_shape = (\n",
    "            metadata['tdef']['count'],\n",
    "            metadata['zdef']['count'],\n",
    "            metadata['num_vars'], \n",
    "            metadata['ydef']['count'],\n",
    "            metadata['xdef']['count']\n",
    "        )\n",
    "        reshaped_data = raw_data.reshape(data_shape, order='C') # Changed from 'F' to 'C'\n",
    "\n",
    "        # 5. Create DataArrays for each variable and populate the Dataset\n",
    "        data_vars = {}\n",
    "        for i, var_info in enumerate(metadata['variables']):\n",
    "            var_data_array = reshaped_data[:, :, i, :, :]\n",
    "            var_data_array[var_data_array == metadata['undef']] = np.nan\n",
    "            squeezed_data = var_data_array.squeeze()\n",
    "            \n",
    "            dims = []\n",
    "            if metadata['tdef']['count'] > 1: dims.append('time')\n",
    "            if metadata['zdef']['count'] > 1: dims.append('z')\n",
    "            dims.append('lat')\n",
    "            dims.append('lon')\n",
    "\n",
    "            current_data_array_coords = {}\n",
    "            if 'time' in dims:\n",
    "                current_data_array_coords['time'] = time_coords\n",
    "            if 'z' in dims:\n",
    "                current_data_array_coords['z'] = z_coords\n",
    "            current_data_array_coords['lat'] = y_coords\n",
    "            current_data_array_coords['lon'] = x_coords\n",
    "\n",
    "            data_vars[var_info['name']] = xr.DataArray(\n",
    "                squeezed_data,\n",
    "                coords=current_data_array_coords,\n",
    "                dims=dims,\n",
    "                name=var_info['name'],\n",
    "                attrs={'long_name': var_info['description'], 'units': 'kg/m^2/s'}\n",
    "            )\n",
    "\n",
    "        # 6. Create the xarray Dataset\n",
    "        ds = xr.Dataset(\n",
    "            data_vars=data_vars,\n",
    "            coords={\n",
    "                'time': time_coords,\n",
    "                'z': z_coords,\n",
    "                'lat': y_coords,\n",
    "                'lon': x_coords\n",
    "            },\n",
    "            attrs={'title': metadata.get('title', 'Converted from GrADS binary')}\n",
    "        )\n",
    "\n",
    "        # Ensure the output directory exists\n",
    "        output_dir = os.path.dirname(output_nc_file)\n",
    "        if output_dir and not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "\n",
    "        # 7. Save to NetCDF\n",
    "        ds.to_netcdf(output_nc_file)\n",
    "        print(f\"Successfully converted '{ctl_file_path}' to '{output_nc_file}'\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Data file '{data_file_path}' not found. Please ensure the .dat file exists and the `dset` path in your .ctl is correct relative to the .ctl file.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during conversion: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "630534b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- How to use ---\n",
    "# IMPORTANT: If you encounter Permission Denied errors, try changing the output directory\n",
    "# to a simple, non-cloud-synced location, e.g., C:/temp/converted_data/\n",
    "\n",
    "ctl_file = \"to_convert/JunIC_nmme_precip_anom_stdanom.ctl\"\n",
    "output_netcdf_file = \"converted/JunIC_nmme_precip_anom_stdanom.nc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f80afcfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Could not parse start date '15Jul2025' with '%HZ%d%b%Y'. Attempting generic parse.\n",
      "File size matches expected data size: 6255360 bytes.\n",
      "Sample of raw data (first 20 elements): [0.24273087 0.24273087 0.24273078 0.24273072 0.2427307  0.24273065\n",
      " 0.24273063 0.24273056 0.24273045 0.24273038 0.24273033 0.24273027\n",
      " 0.2427302  0.2427301  0.24272996 0.24272978 0.2427296  0.24272943\n",
      " 0.24272929 0.24272926]\n",
      "Successfully converted 'to_convert/JunIC_nmme_precip_anom_stdanom.ctl' to 'converted/JunIC_nmme_precip_anom_stdanom.nc'\n"
     ]
    }
   ],
   "source": [
    "# Call the conversion function\n",
    "convert_grads_to_netcdf(ctl_file, output_netcdf_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
